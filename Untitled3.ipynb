{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fffe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9647e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(weight, bias, x):\n",
    "    model = np.add(np.dot(x, weight), bias)\n",
    "    print('model'.format(model))\n",
    "    logit = 1/(1+np.exp(-model))\n",
    "    print('Type:{}'.format(logit))\n",
    "    return np.round(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5ee674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(logictype, weightdict, dataset):\n",
    "    weights = np.array([weightdict[logictype][w] for w in weightdict[logictype].keys()])\n",
    "    output = np.array([perceptron(weights, weightdict['bias'][logictype], val) for val in dataset])\n",
    "    print(logictype)\n",
    "    return logictype, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d05423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    logic = {\n",
    "        'logic_and':{\n",
    "            'w0':-0.1,\n",
    "            'w1':0.2,\n",
    "            'w2':0.2\n",
    "        },\n",
    "        \n",
    "        'logic_nand':{\n",
    "            'w0':0.6,\n",
    "            'w1':-0.8,\n",
    "            'w2':-0.8\n",
    "        },\n",
    "        \n",
    "        'bias':{\n",
    "            'logic_and':-0.2,\n",
    "            'logic_nand':0.3\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    dataset = np.array([\n",
    "        [1,0,0],\n",
    "        [1,0,1],\n",
    "        [1,1,0],\n",
    "        [1,1,1]\n",
    "    ])\n",
    "    \n",
    "    logic_and = compute('logic_and', logic, dataset)\n",
    "    logic_nand = compute('logic_type', logic, dataset)\n",
    "    def template(dataset, name, data):\n",
    "        print(\"X0\\tX1\\tY\")\n",
    "        toprint = [\"{1}\\t{2}\\t{3}\\t{0}\".format(output, *datas) for datas, output in zip(dataset, data)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eee10592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pendes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9cee52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    from keras.models import Sequential\n",
      "from keras.layers import Embedding, SimpleRNN,Dense\n",
      "from keras.datasets import imdb\n",
      "from keras.preprocessing import sequence\n",
      "max_features = 10000\n",
      "maxlen = 500\n",
      "batch_size = 32\n",
      "\n",
      "print('Loading data...')\n",
      "(input_train, y_train), (input_test, y_test) = imdb.load_data( num_words=max_features)\n",
      "print(len(input_train), 'train sequences')\n",
      "print(len(input_test), 'test sequences')\n",
      "print('Pad sequences (samples x time)')\n",
      "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
      "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
      "print('input_train shape:', input_train.shape)\n",
      "print('input_test shape:', input_test.shape)\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Embedding(max_features, 32)) #max features  ye hai agar =10,000  to 32 se multiple karne ke baad  320,000\n",
      "model.add(SimpleRNN(32))               #(32+32+1)*32=2080\n",
      "model.add(Dense(1, activation='sigmoid'))#(32+1)*1=33\n",
      "model.summary()\n",
      "\n",
      "model.compile(optimizer='rmsprop', loss='binary_crossentropy',metrics=['acc'])\n",
      "history = model.fit(input_train, y_train,epochs=10, batch_size=128, validation_split=0.2)\n",
      "\n",
      "predicted_classes = model.predict(input_test)\n",
      "\n",
      "import numpy as np\n",
      "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
      "\n",
      "predicted_classes.shape, y_test.shape\n",
      "\n",
      "correct = np.where(predicted_classes==y_test)[0]\n",
      "print (\"Found %d correct labels\" % len(correct))\n",
      "\n",
      "\n",
      "incorrect = np.where(predicted_classes!=y_test)[0]\n",
      "print (\"Found %d incorrect labels\" % len(incorrect))\n",
      "\n",
      "from sklearn.metrics import classification_report\n",
      "num_classes=2\n",
      "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
      "print(classification_report(y_test, predicted_classes, target_names=target_names))\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "acc = history.history['acc']\n",
      "val_acc = history.history['val_acc']\n",
      "epochs = range(1, len(acc) + 1)\n",
      "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
      "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
      "plt.title('Training and validation accuracy')\n",
      "plt.legend()\n",
      "\n",
      "\n",
      "\n",
      "plt.figure()\n",
      "loss = history.history['loss']\n",
      "val_loss = history.history['val_loss']\n",
      "epochs = range(1, len(acc) + 1)\n",
      "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
      "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
      "plt.title('Training and validation loss')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(print10())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b3511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
